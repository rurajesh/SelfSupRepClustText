{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rurajesh/SelfSupRepClustText/blob/main/SelfSupClusterNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "1S9PoJvR9K-I"
      },
      "id": "1S9PoJvR9K-I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class SelfSupClusterNet(nn.Module):\n",
        "    '''\n",
        "    '''\n",
        "    def __init__(self, AutoModel:preTrainedCondenser, AutoTokenizer: preTrainedCondenserTokenizer, nmb_prototypes, output_dim):\n",
        "        super(SelfSupClusterNet, self).__init__()\n",
        "        # self.a=nn.Parameter(100*torch.randn(1, requires_grad=True, dtype=torch.float32))\n",
        "        # self.b=nn.Parameter(100*torch.randn(1, requires_grad=True, dtype=torch.float32))\n",
        "        # prototype layer\n",
        "        self.model = preTrainedCondenser\n",
        "        self.tokenizer = preTrainedCondenserTokenizer\n",
        "        self.prototypes = None\n",
        "        if isinstance(nmb_prototypes, list):\n",
        "            self.prototypes = MultiPrototypes(output_dim, nmb_prototypes)\n",
        "        elif nmb_prototypes > 0:\n",
        "            self.prototypes = nn.Linear(output_dim, nmb_prototypes, bias=False)\n",
        "\n",
        "        \n",
        "    \n",
        "    def forward(self,inputs):\n",
        "      # model = AutoModel.from_pretrained('Luyu/co-condenser-marco')\n",
        "      # tokenizer = AutoTokenizer.from_pretrained('Luyu/co-condenser-marco')\n",
        "      if not isinstance(inputs, list):\n",
        "          inputs = [inputs]\n",
        "      tokens_batch = self.tokenizer(inputs, return_tensors='pt', padding=True, truncation=True)\n",
        "      print(pt_batch)\n",
        "      # print(model)\n",
        "      embeddings=model(**tokens_batch)\n",
        "      outputs = self.prototypes(cls_token_batch)\n",
        "      return embeddings, outputs\n",
        "\n",
        "\n",
        "\n",
        "class MultiPrototypes(nn.Module):\n",
        "    def __init__(self, output_dim, nmb_prototypes):\n",
        "        super(MultiPrototypes, self).__init__()\n",
        "        self.nmb_heads = len(nmb_prototypes)\n",
        "        for i, k in enumerate(nmb_prototypes):\n",
        "            self.add_module(\"prototypes\" + str(i), nn.Linear(output_dim, k, bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = []\n",
        "        for i in range(self.nmb_heads):\n",
        "            out.append(getattr(self, \"prototypes\" + str(i))(x))\n",
        "        return out"
      ],
      "metadata": {
        "id": "EJ9wA2_axR8k"
      },
      "id": "EJ9wA2_axR8k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "import argparse\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "from logging import getLogger\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import apex\n",
        "from apex.parallel.LARC import LARC\n",
        "\n",
        "from src.utils import (\n",
        "    bool_flag,\n",
        "    initialize_exp,\n",
        "    restart_from_checkpoint,\n",
        "    fix_random_seeds,\n",
        "    AverageMeter,\n",
        "    init_distributed_mode,\n",
        ")\n",
        "from src.multicropdataset import MultiCropDataset\n",
        "import src.resnet50 as resnet_models\n",
        "\n",
        "\n",
        "######### My imports ###########\n",
        "import transformers\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "condenser_model = AutoModel.from_pretrained('Luyu/co-condenser-marco')\n",
        "condenser_tokenizer = AutoTokenizer.from_pretrained('Luyu/co-condenser-marco')\n",
        "output_dim = 768\n",
        "\n",
        "logger = getLogger()\n",
        "\n",
        "# parser = argparse.ArgumentParser(description=\"Implementation of SwAV\")\n",
        "\n",
        "# #########################\n",
        "# #### data parameters ####\n",
        "# #########################\n",
        "# parser.add_argument(\"--data_path\", type=str, default=\"/path/to/imagenet\",\n",
        "#                     help=\"path to dataset repository\")\n",
        "# parser.add_argument(\"--nmb_crops\", type=int, default=[2], nargs=\"+\",\n",
        "#                     help=\"list of number of crops (example: [2, 6])\")\n",
        "# parser.add_argument(\"--size_crops\", type=int, default=[224], nargs=\"+\",\n",
        "#                     help=\"crops resolutions (example: [224, 96])\")\n",
        "# parser.add_argument(\"--min_scale_crops\", type=float, default=[0.14], nargs=\"+\",\n",
        "#                     help=\"argument in RandomResizedCrop (example: [0.14, 0.05])\")\n",
        "# parser.add_argument(\"--max_scale_crops\", type=float, default=[1], nargs=\"+\",\n",
        "#                     help=\"argument in RandomResizedCrop (example: [1., 0.14])\")\n",
        "\n",
        "# #########################\n",
        "# ## swav specific params #\n",
        "# #########################\n",
        "# parser.add_argument(\"--crops_for_assign\", type=int, nargs=\"+\", default=[0, 1],\n",
        "#                     help=\"list of crops id used for computing assignments\")\n",
        "# parser.add_argument(\"--temperature\", default=0.1, type=float,\n",
        "#                     help=\"temperature parameter in training loss\")\n",
        "# parser.add_argument(\"--epsilon\", default=0.05, type=float,\n",
        "#                     help=\"regularization parameter for Sinkhorn-Knopp algorithm\")\n",
        "# parser.add_argument(\"--sinkhorn_iterations\", default=3, type=int,\n",
        "#                     help=\"number of iterations in Sinkhorn-Knopp algorithm\")\n",
        "# parser.add_argument(\"--feat_dim\", default=128, type=int,\n",
        "#                     help=\"feature dimension\")\n",
        "# parser.add_argument(\"--nmb_prototypes\", default=3000, type=int,\n",
        "#                     help=\"number of prototypes\")\n",
        "# parser.add_argument(\"--queue_length\", type=int, default=0,\n",
        "#                     help=\"length of the queue (0 for no queue)\")\n",
        "# parser.add_argument(\"--epoch_queue_starts\", type=int, default=15,\n",
        "#                     help=\"from this epoch, we start using a queue\")\n",
        "\n",
        "# #########################\n",
        "# #### optim parameters ###\n",
        "# #########################\n",
        "# parser.add_argument(\"--epochs\", default=100, type=int,\n",
        "#                     help=\"number of total epochs to run\")\n",
        "# parser.add_argument(\"--batch_size\", default=64, type=int,\n",
        "#                     help=\"batch size per gpu, i.e. how many unique instances per gpu\")\n",
        "# parser.add_argument(\"--base_lr\", default=4.8, type=float, help=\"base learning rate\")\n",
        "# parser.add_argument(\"--final_lr\", type=float, default=0, help=\"final learning rate\")\n",
        "# parser.add_argument(\"--freeze_prototypes_niters\", default=313, type=int,\n",
        "#                     help=\"freeze the prototypes during this many iterations from the start\")\n",
        "# parser.add_argument(\"--wd\", default=1e-6, type=float, help=\"weight decay\")\n",
        "# parser.add_argument(\"--warmup_epochs\", default=10, type=int, help=\"number of warmup epochs\")\n",
        "# parser.add_argument(\"--start_warmup\", default=0, type=float,\n",
        "#                     help=\"initial warmup learning rate\")\n",
        "\n",
        "# #########################\n",
        "# #### dist parameters ###\n",
        "# #########################\n",
        "# parser.add_argument(\"--dist_url\", default=\"env://\", type=str, help=\"\"\"url used to set up distributed\n",
        "#                     training; see https://pytorch.org/docs/stable/distributed.html\"\"\")\n",
        "# parser.add_argument(\"--world_size\", default=-1, type=int, help=\"\"\"\n",
        "#                     number of processes: it is set automatically and\n",
        "#                     should not be passed as argument\"\"\")\n",
        "# parser.add_argument(\"--rank\", default=0, type=int, help=\"\"\"rank of this process:\n",
        "#                     it is set automatically and should not be passed as argument\"\"\")\n",
        "# parser.add_argument(\"--local_rank\", default=0, type=int,\n",
        "#                     help=\"this argument is not used and should be ignored\")\n",
        "\n",
        "# #########################\n",
        "# #### other parameters ###\n",
        "# #########################\n",
        "# parser.add_argument(\"--arch\", default=\"resnet50\", type=str, help=\"convnet architecture\")\n",
        "# parser.add_argument(\"--hidden_mlp\", default=2048, type=int,\n",
        "#                     help=\"hidden layer dimension in projection head\")\n",
        "# parser.add_argument(\"--workers\", default=10, type=int,\n",
        "#                     help=\"number of data loading workers\")\n",
        "# parser.add_argument(\"--checkpoint_freq\", type=int, default=25,\n",
        "#                     help=\"Save the model periodically\")\n",
        "# parser.add_argument(\"--use_fp16\", type=bool_flag, default=True,\n",
        "#                     help=\"whether to train with mixed precision or not\")\n",
        "# parser.add_argument(\"--sync_bn\", type=str, default=\"pytorch\", help=\"synchronize bn\")\n",
        "# parser.add_argument(\"--syncbn_process_group_size\", type=int, default=8, help=\"\"\" see\n",
        "#                     https://github.com/NVIDIA/apex/blob/master/apex/parallel/__init__.py#L58-L67\"\"\")\n",
        "# parser.add_argument(\"--dump_path\", type=str, default=\".\",\n",
        "#                     help=\"experiment dump path for checkpoints and log\")\n",
        "# parser.add_argument(\"--seed\", type=int, default=31, help=\"seed\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    global args\n",
        "    args = parser.parse_args()\n",
        "    init_distributed_mode(args)\n",
        "    fix_random_seeds(args.seed)\n",
        "    logger, training_stats = initialize_exp(args, \"epoch\", \"loss\")\n",
        "\n",
        "    # build data\n",
        "    train_dataset = MultiCropDataset(\n",
        "        args.data_path,\n",
        "        args.size_crops,\n",
        "        args.nmb_crops,\n",
        "        args.min_scale_crops,\n",
        "        args.max_scale_crops,\n",
        "    )\n",
        "    sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        sampler=sampler,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    logger.info(\"Building data done with {} images loaded.\".format(len(train_dataset)))\n",
        "\n",
        "    # build model\n",
        "    # model = resnet_models.__dict__[args.arch](\n",
        "    #     normalize=True,\n",
        "    #     hidden_mlp=args.hidden_mlp,\n",
        "    #     output_dim=args.feat_dim,\n",
        "    #     nmb_prototypes=args.nmb_prototypes,\n",
        "    # )\n",
        "\n",
        "    model = SelfSupClusterNet( \n",
        "        preTrainedCondenser=condenser_model,\n",
        "        PreTrainedCondenserTokenizer=condenser_tokenizer,\n",
        "        nmb_prototypes=1000,\n",
        "        output_dim=output_dim)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    # synchronize batch norm layers\n",
        "    if args.sync_bn == \"pytorch\":\n",
        "        model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
        "    elif args.sync_bn == \"apex\":\n",
        "        # with apex syncbn we sync bn per group because it speeds up computation\n",
        "        # compared to global syncbn\n",
        "        process_group = apex.parallel.create_syncbn_process_group(args.syncbn_process_group_size)\n",
        "        model = apex.parallel.convert_syncbn_model(model, process_group=process_group)\n",
        "    # copy model to GPU\n",
        "    model = model.cuda()\n",
        "    if args.rank == 0:\n",
        "        logger.info(model)\n",
        "    logger.info(\"Building model done.\")\n",
        "\n",
        "    # build optimizer\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=args.base_lr,\n",
        "        momentum=0.9,\n",
        "        weight_decay=args.wd,\n",
        "    )\n",
        "    optimizer = LARC(optimizer=optimizer, trust_coefficient=0.001, clip=False)\n",
        "    warmup_lr_schedule = np.linspace(args.start_warmup, args.base_lr, len(train_loader) * args.warmup_epochs)\n",
        "    iters = np.arange(len(train_loader) * (args.epochs - args.warmup_epochs))\n",
        "    cosine_lr_schedule = np.array([args.final_lr + 0.5 * (args.base_lr - args.final_lr) * (1 + \\\n",
        "                         math.cos(math.pi * t / (len(train_loader) * (args.epochs - args.warmup_epochs)))) for t in iters])\n",
        "    lr_schedule = np.concatenate((warmup_lr_schedule, cosine_lr_schedule))\n",
        "    logger.info(\"Building optimizer done.\")\n",
        "\n",
        "    # init mixed precision\n",
        "    if args.use_fp16:\n",
        "        model, optimizer = apex.amp.initialize(model, optimizer, opt_level=\"O1\")\n",
        "        logger.info(\"Initializing mixed precision done.\")\n",
        "\n",
        "    # wrap model\n",
        "    model = nn.parallel.DistributedDataParallel(\n",
        "        model,\n",
        "        device_ids=[args.gpu_to_work_on]\n",
        "    )\n",
        "\n",
        "    # optionally resume from a checkpoint\n",
        "    to_restore = {\"epoch\": 0}\n",
        "    restart_from_checkpoint(\n",
        "        os.path.join(args.dump_path, \"checkpoint.pth.tar\"),\n",
        "        run_variables=to_restore,\n",
        "        state_dict=model,\n",
        "        optimizer=optimizer,\n",
        "        amp=apex.amp,\n",
        "    )\n",
        "    start_epoch = to_restore[\"epoch\"]\n",
        "\n",
        "    # build the queue\n",
        "    queue = None\n",
        "    queue_path = os.path.join(args.dump_path, \"queue\" + str(args.rank) + \".pth\")\n",
        "    if os.path.isfile(queue_path):\n",
        "        queue = torch.load(queue_path)[\"queue\"]\n",
        "    # the queue needs to be divisible by the batch size\n",
        "    args.queue_length -= args.queue_length % (args.batch_size * args.world_size)\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "        # train the network for one epoch\n",
        "        logger.info(\"============ Starting epoch %i ... ============\" % epoch)\n",
        "\n",
        "        # set sampler\n",
        "        train_loader.sampler.set_epoch(epoch)\n",
        "\n",
        "        # optionally starts a queue\n",
        "        if args.queue_length > 0 and epoch >= args.epoch_queue_starts and queue is None:\n",
        "            queue = torch.zeros(\n",
        "                len(args.crops_for_assign),\n",
        "                args.queue_length // args.world_size,\n",
        "                args.feat_dim,\n",
        "            ).cuda()\n",
        "\n",
        "        # train the network\n",
        "        scores, queue = train(train_loader, model, optimizer, epoch, lr_schedule, queue)\n",
        "        training_stats.update(scores)\n",
        "\n",
        "        # save checkpoints\n",
        "        if args.rank == 0:\n",
        "            save_dict = {\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            if args.use_fp16:\n",
        "                save_dict[\"amp\"] = apex.amp.state_dict()\n",
        "            torch.save(\n",
        "                save_dict,\n",
        "                os.path.join(args.dump_path, \"checkpoint.pth.tar\"),\n",
        "            )\n",
        "            if epoch % args.checkpoint_freq == 0 or epoch == args.epochs - 1:\n",
        "                shutil.copyfile(\n",
        "                    os.path.join(args.dump_path, \"checkpoint.pth.tar\"),\n",
        "                    os.path.join(args.dump_checkpoints, \"ckp-\" + str(epoch) + \".pth\"),\n",
        "                )\n",
        "        if queue is not None:\n",
        "            torch.save({\"queue\": queue}, queue_path)\n",
        "\n",
        "\n",
        "def train(train_loader, model, optimizer, epoch, lr_schedule, queue):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    model.train()\n",
        "    use_the_queue = False\n",
        "\n",
        "    end = time.time()\n",
        "    for it, inputs in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        # update learning rate\n",
        "        iteration = epoch * len(train_loader) + it\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr_schedule[iteration]\n",
        "\n",
        "        # normalize the prototypes\n",
        "        with torch.no_grad():\n",
        "            w = model.module.prototypes.weight.data.clone()\n",
        "            w = nn.functional.normalize(w, dim=1, p=2)\n",
        "            model.module.prototypes.weight.copy_(w)\n",
        "\n",
        "        # ============ multi-res forward passes ... ============\n",
        "        embedding, output = model(inputs)\n",
        "        embedding = embedding.detach()\n",
        "        bs = inputs[0].size(0)\n",
        "\n",
        "        # ============ swav loss ... ============\n",
        "        loss = 0\n",
        "        for i, crop_id in enumerate(args.crops_for_assign):\n",
        "            with torch.no_grad():\n",
        "                out = output[bs * crop_id: bs * (crop_id + 1)].detach()\n",
        "\n",
        "                # time to use the queue\n",
        "                if queue is not None:\n",
        "                    if use_the_queue or not torch.all(queue[i, -1, :] == 0):\n",
        "                        use_the_queue = True\n",
        "                        out = torch.cat((torch.mm(\n",
        "                            queue[i],\n",
        "                            model.module.prototypes.weight.t()\n",
        "                        ), out))\n",
        "                    # fill the queue\n",
        "                    queue[i, bs:] = queue[i, :-bs].clone()\n",
        "                    queue[i, :bs] = embedding[crop_id * bs: (crop_id + 1) * bs]\n",
        "\n",
        "                # get assignments\n",
        "                q = distributed_sinkhorn(out)[-bs:]\n",
        "\n",
        "            # cluster assignment prediction\n",
        "            subloss = 0\n",
        "            for v in np.delete(np.arange(np.sum(args.nmb_crops)), crop_id):\n",
        "                x = output[bs * v: bs * (v + 1)] / args.temperature\n",
        "                subloss -= torch.mean(torch.sum(q * F.log_softmax(x, dim=1), dim=1))\n",
        "            loss += subloss / (np.sum(args.nmb_crops) - 1)\n",
        "        loss /= len(args.crops_for_assign)\n",
        "\n",
        "        # ============ backward and optim step ... ============\n",
        "        optimizer.zero_grad()\n",
        "        if args.use_fp16:\n",
        "            with apex.amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "        # cancel gradients for the prototypes\n",
        "        if iteration < args.freeze_prototypes_niters:\n",
        "            for name, p in model.named_parameters():\n",
        "                if \"prototypes\" in name:\n",
        "                    p.grad = None\n",
        "        optimizer.step()\n",
        "\n",
        "        # ============ misc ... ============\n",
        "        losses.update(loss.item(), inputs[0].size(0))\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        if args.rank ==0 and it % 50 == 0:\n",
        "            logger.info(\n",
        "                \"Epoch: [{0}][{1}]\\t\"\n",
        "                \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
        "                \"Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n",
        "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\"\n",
        "                \"Lr: {lr:.4f}\".format(\n",
        "                    epoch,\n",
        "                    it,\n",
        "                    batch_time=batch_time,\n",
        "                    data_time=data_time,\n",
        "                    loss=losses,\n",
        "                    lr=optimizer.optim.param_groups[0][\"lr\"],\n",
        "                )\n",
        "            )\n",
        "    return (epoch, losses.avg), queue\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def distributed_sinkhorn(out):\n",
        "    Q = torch.exp(out / args.epsilon).t() # Q is K-by-B for consistency with notations from our paper\n",
        "    B = Q.shape[1] * args.world_size # number of samples to assign\n",
        "    K = Q.shape[0] # how many prototypes\n",
        "\n",
        "    # make the matrix sums to 1\n",
        "    sum_Q = torch.sum(Q)\n",
        "    dist.all_reduce(sum_Q)\n",
        "    Q /= sum_Q\n",
        "\n",
        "    for it in range(args.sinkhorn_iterations):\n",
        "        # normalize each row: total weight per prototype must be 1/K\n",
        "        sum_of_rows = torch.sum(Q, dim=1, keepdim=True)\n",
        "        dist.all_reduce(sum_of_rows)\n",
        "        Q /= sum_of_rows\n",
        "        Q /= K\n",
        "\n",
        "        # normalize each column: total weight per sample must be 1/B\n",
        "        Q /= torch.sum(Q, dim=0, keepdim=True)\n",
        "        Q /= B\n",
        "\n",
        "    Q *= B # the colomns must sum to 1 so that Q is an assignment\n",
        "    return Q.t()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "aZYOtuj6P-kq"
      },
      "id": "aZYOtuj6P-kq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MSGPlK_fvgXq"
      },
      "id": "MSGPlK_fvgXq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "SelfSupClusterNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}